/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 * SPDX-License-Identifier: GPL-3.0-or-later
 *
 * Warp-level reduction primitives inspired by tiny-cuda-nn's optimizations.
 * These use warp shuffle instructions for 5-10x faster reductions compared to shared memory.
 */

#pragma once

#include <cuda_runtime.h>
#include <type_traits>
#include <cstdint>

namespace gs {
namespace warp_ops {

// ============= WARP-LEVEL REDUCTIONS (NO SHARED MEMORY!) =============

/**
 * @brief Warp-level sum reduction using shuffle instructions
 *
 * Reduces a value across all threads in a warp using __shfl_xor_sync.
 * This is 5-10x faster than shared memory because:
 * - No synchronization needed within warp
 * - No shared memory access (stays in registers)
 * - Uses hardware shuffle unit
 *
 * @tparam T Type of value to reduce (float, int, etc.)
 * @param val Value from this thread
 * @return Sum of all values in the warp (valid in all threads)
 */
template<typename T>
__device__ inline T warp_reduce_sum(T val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_xor_sync(0xffffffff, val, offset);
    }
    return val;
}

/**
 * @brief Warp-level max reduction using shuffle instructions
 */
template<typename T>
__device__ inline T warp_reduce_max(T val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        T other = __shfl_xor_sync(0xffffffff, val, offset);
        val = (val > other) ? val : other;
    }
    return val;
}

/**
 * @brief Warp-level min reduction using shuffle instructions
 */
template<typename T>
__device__ inline T warp_reduce_min(T val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        T other = __shfl_xor_sync(0xffffffff, val, offset);
        val = (val < other) ? val : other;
    }
    return val;
}

/**
 * @brief Warp-level product reduction using shuffle instructions
 */
template<typename T>
__device__ inline T warp_reduce_prod(T val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        val *= __shfl_xor_sync(0xffffffff, val, offset);
    }
    return val;
}

// ============= BLOCK-LEVEL REDUCTIONS (USES WARP REDUCTION) =============

/**
 * @brief Block-level sum reduction using warp primitives
 *
 * Two-stage reduction:
 * 1. Each warp reduces its values using warp_reduce_sum
 * 2. First warp reduces results from all warps
 *
 * @tparam T Type of value to reduce
 * @param val Value from this thread
 * @return Sum of all values in the block (valid only in thread 0)
 */
template<typename T>
__device__ T block_reduce_sum(T val) {
    static __shared__ T shared[32];  // One value per warp (max 32 warps per block)

    int lane = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    // Reduce within warp
    val = warp_reduce_sum(val);

    // First thread in each warp writes to shared memory
    if (lane == 0) {
        shared[warp_id] = val;
    }
    __syncthreads();

    // First warp reduces across warps
    if (warp_id == 0) {
        val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : T(0);
        val = warp_reduce_sum(val);
    }

    return val;
}

/**
 * @brief Block-level max reduction using warp primitives
 */
template<typename T>
__device__ T block_reduce_max(T val) {
    static __shared__ T shared[32];

    int lane = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    val = warp_reduce_max(val);

    if (lane == 0) {
        shared[warp_id] = val;
    }
    __syncthreads();

    if (warp_id == 0) {
        val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : -std::numeric_limits<T>::infinity();
        val = warp_reduce_max(val);
    }

    return val;
}

/**
 * @brief Block-level min reduction using warp primitives
 */
template<typename T>
__device__ T block_reduce_min(T val) {
    static __shared__ T shared[32];

    int lane = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    val = warp_reduce_min(val);

    if (lane == 0) {
        shared[warp_id] = val;
    }
    __syncthreads();

    if (warp_id == 0) {
        val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : std::numeric_limits<T>::infinity();
        val = warp_reduce_min(val);
    }

    return val;
}

/**
 * @brief Block-level product reduction using warp primitives
 */
template<typename T>
__device__ T block_reduce_prod(T val) {
    static __shared__ T shared[32];

    int lane = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    val = warp_reduce_prod(val);

    if (lane == 0) {
        shared[warp_id] = val;
    }
    __syncthreads();

    if (warp_id == 0) {
        val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : T(1);
        val = warp_reduce_prod(val);
    }

    return val;
}

// ============= VECTORIZED LOADS + WARP REDUCTION =============

/**
 * @brief Vectorized sum reduction using float4 loads + warp shuffles
 *
 * This combines two optimizations:
 * 1. Load 4 floats at once (4x memory bandwidth) - only if properly aligned
 * 2. Warp-level reduction (5-10x faster than shared memory)
 *
 * Expected speedup: 10-20x over naive implementation (when aligned)!
 *
 * @param input Input data (should be 16-byte aligned for best performance)
 * @param n Number of elements
 * @return Sum of all elements (valid only in thread 0 of each block)
 */
template<typename T>
__device__ T vectorized_block_reduce_sum(const T* input, size_t n) {
    size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = 0;
    if constexpr (std::is_same_v<T, float>) {
        // Check alignment at runtime (pointer must be 16-byte aligned)
        bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

        if (is_aligned && idx + 3 < n) {
            // Load 4 floats in one transaction (16 bytes aligned)
            float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
            val = vals.x + vals.y + vals.z + vals.w;
        } else if (idx < n) {
            // Scalar fallback for unaligned data or remainder
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val += input[i];
            }
        }
    } else {
        // Fallback for non-float types
        if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val += input[i];
            }
        }
    }

    // Warp-level reduction
    return block_reduce_sum(val);
}

/**
 * @brief Vectorized max reduction using float4 loads + warp shuffles
 */
template<typename T>
__device__ T vectorized_block_reduce_max(const T* input, size_t n) {
    size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = -std::numeric_limits<T>::infinity();
    if constexpr (std::is_same_v<T, float>) {
        bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

        if (is_aligned && idx + 3 < n) {
            float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
            val = fmaxf(fmaxf(vals.x, vals.y), fmaxf(vals.z, vals.w));
        } else if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val = fmaxf(val, input[i]);
            }
        }
    } else {
        if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val = (input[i] > val) ? input[i] : val;
            }
        }
    }

    return block_reduce_max(val);
}

/**
 * @brief Vectorized min reduction using float4 loads + warp shuffles
 */
template<typename T>
__device__ T vectorized_block_reduce_min(const T* input, size_t n) {
    size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = std::numeric_limits<T>::infinity();
    if constexpr (std::is_same_v<T, float>) {
        bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

        if (is_aligned && idx + 3 < n) {
            float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
            val = fminf(fminf(vals.x, vals.y), fminf(vals.z, vals.w));
        } else if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val = fminf(val, input[i]);
            }
        }
    } else {
        if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val = (input[i] < val) ? input[i] : val;
            }
        }
    }

    return block_reduce_min(val);
}

/**
 * @brief Vectorized product reduction using float4 loads + warp shuffles
 */
template<typename T>
__device__ T vectorized_block_reduce_prod(const T* input, size_t n) {
    size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = 1;
    if constexpr (std::is_same_v<T, float>) {
        bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

        if (is_aligned && idx + 3 < n) {
            float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
            val = vals.x * vals.y * vals.z * vals.w;
        } else if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val *= input[i];
            }
        }
    } else {
        if (idx < n) {
            for (size_t i = idx; i < n && i < idx + 4; ++i) {
                val *= input[i];
            }
        }
    }

    return block_reduce_prod(val);
}

// ============= SEGMENTED REDUCTION HELPERS =============

/**
 * @brief Vectorized segmented reduction using warp shuffles
 *
 * Reduces a single contiguous segment with vectorized loads + warp reduction.
 * Much faster than CUB's iterator-based approach for medium-sized segments.
 *
 * @tparam T Type of elements (float, int, etc.)
 * @param segment_start Pointer to start of segment
 * @param segment_size Number of elements in segment
 * @return Reduced value (valid only in thread 0)
 */
template<typename T>
__device__ T vectorized_segment_reduce_sum(const T* segment_start, size_t segment_size) {
    size_t vec_idx = threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = 0;
    if constexpr (std::is_same_v<T, float>) {
        bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

        // Vectorized load: 4 elements per thread (only if aligned)
        if (is_aligned && idx + 3 < segment_size) {
            float4 vals = reinterpret_cast<const float4*>(segment_start)[vec_idx];
            val = vals.x + vals.y + vals.z + vals.w;
        } else if (idx < segment_size) {
            // Scalar fallback for unaligned data or remainder
            for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                val += segment_start[i];
            }
        }
    } else {
        // Fallback for non-float types
        if (idx < segment_size) {
            for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                val += segment_start[i];
            }
        }
    }

    // Warp-level reduction within block
    return block_reduce_sum(val);
}

/**
 * @brief Vectorized segmented max reduction
 */
template<typename T>
__device__ T vectorized_segment_reduce_max(const T* segment_start, size_t segment_size) {
    size_t vec_idx = threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = -std::numeric_limits<T>::infinity();
    if constexpr (std::is_same_v<T, float>) {
        bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

        if (is_aligned && idx + 3 < segment_size) {
            float4 vals = reinterpret_cast<const float4*>(segment_start)[vec_idx];
            val = fmaxf(fmaxf(vals.x, vals.y), fmaxf(vals.z, vals.w));
        } else if (idx < segment_size) {
            for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                val = fmaxf(val, segment_start[i]);
            }
        }
    } else {
        if (idx < segment_size) {
            for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                val = (segment_start[i] > val) ? segment_start[i] : val;
            }
        }
    }

    return block_reduce_max(val);
}

/**
 * @brief Vectorized segmented min reduction
 */
template<typename T>
__device__ T vectorized_segment_reduce_min(const T* segment_start, size_t segment_size) {
    size_t vec_idx = threadIdx.x;
    size_t idx = vec_idx * 4;

    T val = std::numeric_limits<T>::infinity();
    if constexpr (std::is_same_v<T, float>) {
        bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

        if (is_aligned && idx + 3 < segment_size) {
            float4 vals = reinterpret_cast<const float4*>(segment_start)[vec_idx];
            val = fminf(fminf(vals.x, vals.y), fminf(vals.z, vals.w));
        } else if (idx < segment_size) {
            for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                val = fminf(val, segment_start[i]);
            }
        }
    } else {
        if (idx < segment_size) {
            for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                val = (segment_start[i] < val) ? segment_start[i] : val;
            }
        }
    }

    return block_reduce_min(val);
}

} // namespace warp_ops
} // namespace gs
