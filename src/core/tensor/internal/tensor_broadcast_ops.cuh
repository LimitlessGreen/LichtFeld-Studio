/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 * SPDX-License-Identifier: GPL-3.0-or-later */

#pragma once

#include "core/logger.hpp"
#include "internal/memory_pool.hpp"
#include <cuda_runtime.h>
#include <type_traits>

namespace gs::tensor_ops {

    // ============================================================================
    // BROADCAST PATTERN DETECTION (tiny-cuda-nn style optimization)
    // ============================================================================

    enum class BroadcastPattern {
        Scalar,           // One operand is scalar (size 1)
        Row,              // e.g., (M×N) op (1×N) - broadcast along rows
        Column,           // e.g., (M×N) op (M×1) - broadcast along columns
        Channel3D,        // e.g., (H×W×C) op (1×1×C) - channel-wise broadcast (neural rendering)
        BatchBroadcast3D, // e.g., (B×H×W) op (1×H×W) - batch broadcast (neural networks)
        Generic           // Complex pattern - use generic kernel
    };

    // Detect broadcast pattern for binary operations
    inline BroadcastPattern detect_broadcast_pattern(
        const size_t* a_shape, const size_t* b_shape,
        size_t a_rank, size_t b_rank, size_t c_rank) {

        // Scalar pattern: One operand has size 1
        size_t a_size = 1, b_size = 1;
        for (size_t i = 0; i < a_rank; ++i)
            a_size *= a_shape[i];
        for (size_t i = 0; i < b_rank; ++i)
            b_size *= b_shape[i];

        if (a_size == 1 || b_size == 1) {
            return BroadcastPattern::Scalar;
        }

        // 3D channel-wise pattern: (H×W×C) op (1×1×C)
        if (c_rank == 3 && a_rank == 3 && b_rank == 3) {
            // Check if one operand is (H×W×C) and the other is (1×1×C)
            if (a_shape[0] > 1 && a_shape[1] > 1 &&
                b_shape[0] == 1 && b_shape[1] == 1 &&
                a_shape[2] == b_shape[2]) {
                return BroadcastPattern::Channel3D;
            }
            if (b_shape[0] > 1 && b_shape[1] > 1 &&
                a_shape[0] == 1 && a_shape[1] == 1 &&
                a_shape[2] == b_shape[2]) {
                return BroadcastPattern::Channel3D;
            }
        }

        // 3D batch broadcast pattern: (B×H×W) op (1×H×W)
        // Common in neural networks: batch norm, instance norm, etc.
        if (c_rank == 3 && a_rank == 3 && b_rank == 3) {
            // Check if one operand is (B×H×W) and the other is (1×H×W)
            if (a_shape[0] > 1 && a_shape[1] > 1 && a_shape[2] > 1 &&
                b_shape[0] == 1 && a_shape[1] == b_shape[1] && a_shape[2] == b_shape[2]) {
                return BroadcastPattern::BatchBroadcast3D;
            }
            if (b_shape[0] > 1 && b_shape[1] > 1 && b_shape[2] > 1 &&
                a_shape[0] == 1 && a_shape[1] == b_shape[1] && a_shape[2] == b_shape[2]) {
                return BroadcastPattern::BatchBroadcast3D;
            }
        }

        // 2D patterns only
        if (c_rank != 2)
            return BroadcastPattern::Generic;

        // Row broadcast: (M×N) op (1×N)
        if (a_rank == 2 && b_rank == 2) {
            if (a_shape[0] > 1 && b_shape[0] == 1 && a_shape[1] == b_shape[1]) {
                return BroadcastPattern::Row;
            }
            if (b_shape[0] > 1 && a_shape[0] == 1 && a_shape[1] == b_shape[1]) {
                return BroadcastPattern::Row;
            }
        }

        // Column broadcast: (M×N) op (M×1)
        if (a_rank == 2 && b_rank == 2) {
            if (a_shape[1] > 1 && b_shape[1] == 1 && a_shape[0] == b_shape[0]) {
                return BroadcastPattern::Column;
            }
            if (b_shape[1] > 1 && a_shape[1] == 1 && a_shape[0] == b_shape[0]) {
                return BroadcastPattern::Column;
            }
        }

        return BroadcastPattern::Generic;
    }

    // ============================================================================
    // SPECIALIZED BROADCAST KERNELS (tiny-cuda-nn style vectorized)
    // ============================================================================

#ifdef __CUDACC__ // Only compile CUDA kernels with nvcc

    // Scalar broadcast kernel - Simple version without float4 (safer)
    template <typename BinaryOp>
    __global__ void broadcast_scalar_kernel_float(
        const float* a, const float* b, float* c,
        size_t a_size, size_t b_size, size_t c_elements,
        BinaryOp op) {
        const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx >= c_elements)
            return;

        const float scalar_val = (a_size == 1) ? a[0] : b[0];
        const float* array = (a_size == 1) ? b : a;
        const bool a_is_scalar = (a_size == 1);

        c[idx] = a_is_scalar ? op(scalar_val, array[idx]) : op(array[idx], scalar_val);
    }

    // Row broadcast kernel - VECTORIZED with float4 (2-4x faster!)
    template <typename BinaryOp>
    __global__ void broadcast_row_kernel_float(
        const float* a, const float* b, float* c,
        size_t M, size_t N, bool a_is_row,
        BinaryOp op) {
        const size_t row = blockIdx.y;
        const size_t col_vec = blockIdx.x * blockDim.x + threadIdx.x;
        const size_t col = col_vec * 4;

        const float* row_data = a_is_row ? b : a;
        const float* broadcast_row = a_is_row ? a : b;
        const size_t row_offset = row * N;

        // Check alignment for float4 vectorization
        bool rows_aligned = (reinterpret_cast<uintptr_t>(&broadcast_row[0]) % 16) == 0;
        bool data_aligned = (reinterpret_cast<uintptr_t>(&row_data[row_offset]) % 16) == 0;
        bool out_aligned = (reinterpret_cast<uintptr_t>(&c[row_offset]) % 16) == 0;

        // Vectorized path: Load 4 floats in one transaction
        if (rows_aligned && data_aligned && out_aligned && col + 3 < N) {
            float4 broadcast_vals = reinterpret_cast<const float4*>(broadcast_row)[col_vec];
            float4 data_vals = reinterpret_cast<const float4*>(&row_data[row_offset])[col_vec];

            float4 result;
            if (a_is_row) {
                result.x = op(broadcast_vals.x, data_vals.x);
                result.y = op(broadcast_vals.y, data_vals.y);
                result.z = op(broadcast_vals.z, data_vals.z);
                result.w = op(broadcast_vals.w, data_vals.w);
            } else {
                result.x = op(data_vals.x, broadcast_vals.x);
                result.y = op(data_vals.y, broadcast_vals.y);
                result.z = op(data_vals.z, broadcast_vals.z);
                result.w = op(data_vals.w, broadcast_vals.w);
            }

            reinterpret_cast<float4*>(&c[row_offset])[col_vec] = result;
        }
        // Scalar fallback for unaligned data or remainder
        else if (col < N) {
            for (size_t i = col; i < N && i < col + 4; ++i) {
                const size_t idx = row_offset + i;
                c[idx] = a_is_row ? op(broadcast_row[i], row_data[idx]) : op(row_data[idx], broadcast_row[i]);
            }
        }
    }

    // Row broadcast COMPARISON kernel - VECTORIZED (float->unsigned char)
    // Optimized for comparison operations: <, >, ==, !=
    // Uses float4 loads for inputs, uchar4 stores for outputs
    template <typename BinaryOp>
    __global__ void broadcast_row_comparison_kernel(
        const float* a, const float* b, unsigned char* c,
        size_t M, size_t N, bool a_is_row,
        BinaryOp op) {
        const size_t row = blockIdx.y;
        const size_t col_vec = blockIdx.x * blockDim.x + threadIdx.x;
        const size_t col = col_vec * 4;

        const float* row_data = a_is_row ? b : a;
        const float* broadcast_row = a_is_row ? a : b;
        const size_t row_offset = row * N;

        // Check alignment: float4 for inputs (16 bytes), uchar4 for output (4 bytes)
        bool rows_aligned = (reinterpret_cast<uintptr_t>(&broadcast_row[0]) % 16) == 0;
        bool data_aligned = (reinterpret_cast<uintptr_t>(&row_data[row_offset]) % 16) == 0;
        bool out_aligned = (reinterpret_cast<uintptr_t>(&c[row_offset]) % 4) == 0;

        // Vectorized path: Load 4 floats, store 4 bytes
        if (rows_aligned && data_aligned && out_aligned && col + 3 < N) {
            // Load 4 floats from each input (16 bytes)
            float4 broadcast_vals = reinterpret_cast<const float4*>(broadcast_row)[col_vec];
            float4 data_vals = reinterpret_cast<const float4*>(&row_data[row_offset])[col_vec];

            // Apply comparison to all 4 values (returns unsigned char)
            uchar4 result;
            if (a_is_row) {
                result.x = op(broadcast_vals.x, data_vals.x);
                result.y = op(broadcast_vals.y, data_vals.y);
                result.z = op(broadcast_vals.z, data_vals.z);
                result.w = op(broadcast_vals.w, data_vals.w);
            } else {
                result.x = op(data_vals.x, broadcast_vals.x);
                result.y = op(data_vals.y, broadcast_vals.y);
                result.z = op(data_vals.z, broadcast_vals.z);
                result.w = op(data_vals.w, broadcast_vals.w);
            }

            // Store 4 bytes in one transaction
            reinterpret_cast<uchar4*>(&c[row_offset])[col_vec] = result;
        }
        // Scalar fallback for unaligned data or remainder
        else if (col < N) {
            for (size_t i = col; i < N && i < col + 4; ++i) {
                const size_t idx = row_offset + i;
                c[idx] = a_is_row ? op(broadcast_row[i], row_data[idx]) : op(row_data[idx], broadcast_row[i]);
            }
        }
    }

    // Column broadcast kernel - Simple version without float4
    template <typename BinaryOp>
    __global__ void broadcast_column_kernel_float(
        const float* a, const float* b, float* c,
        size_t M, size_t N, bool a_is_col,
        BinaryOp op) {
        const size_t row = blockIdx.y;
        const size_t col = blockIdx.x * blockDim.x + threadIdx.x;

        if (col >= N)
            return;

        const size_t idx = row * N + col;
        const float* row_data = a_is_col ? b : a;
        const float col_val = a_is_col ? a[row] : b[row];

        c[idx] = a_is_col ? op(col_val, row_data[idx]) : op(row_data[idx], col_val);
    }

    // Channel3D broadcast kernel - HIGHLY OPTIMIZED for neural rendering
    // Pattern: (H×W×C) op (1×1×C) - extremely common in neural rendering!
    // Uses float4 for RGBA (C=4), manual unrolling for RGB (C=3)
    template <typename BinaryOp>
    __global__ void broadcast_channel3d_kernel_float(
        const float* a, const float* b, float* c,
        size_t H, size_t W, size_t C, bool a_is_broadcast,
        BinaryOp op) {
        // 2D grid: process pixels in parallel
        const size_t pixel_idx = (blockIdx.y * gridDim.x + blockIdx.x) * blockDim.x + threadIdx.x;
        const size_t total_pixels = H * W;

        if (pixel_idx >= total_pixels)
            return;

        const size_t pixel_offset = pixel_idx * C;
        const float* pixel_data = a_is_broadcast ? b + pixel_offset : a + pixel_offset;
        const float* channel_vals = a_is_broadcast ? a : b;

        // FAST PATH: RGBA with float4 vectorization (most efficient!)
        if (C == 4) {
            // Check if pointers are 16-byte aligned for float4
            bool pixel_aligned = (reinterpret_cast<uintptr_t>(pixel_data) % 16) == 0;
            bool chan_aligned = (reinterpret_cast<uintptr_t>(channel_vals) % 16) == 0;
            bool out_aligned = (reinterpret_cast<uintptr_t>(&c[pixel_offset]) % 16) == 0;

            if (pixel_aligned && chan_aligned && out_aligned) {
                // Vectorized load: 4 floats in one transaction!
                float4 pixel_vals = *reinterpret_cast<const float4*>(pixel_data);
                float4 chan_vals = *reinterpret_cast<const float4*>(channel_vals);

                float4 result;
                if (a_is_broadcast) {
                    result.x = op(chan_vals.x, pixel_vals.x);
                    result.y = op(chan_vals.y, pixel_vals.y);
                    result.z = op(chan_vals.z, pixel_vals.z);
                    result.w = op(chan_vals.w, pixel_vals.w);
                } else {
                    result.x = op(pixel_vals.x, chan_vals.x);
                    result.y = op(pixel_vals.y, chan_vals.y);
                    result.z = op(pixel_vals.z, chan_vals.z);
                    result.w = op(pixel_vals.w, chan_vals.w);
                }

                *reinterpret_cast<float4*>(&c[pixel_offset]) = result;
                return;
            }
            // Fallback to scalar for unaligned RGBA
        }

        // FAST PATH: RGB (most common case) - fully unrolled
        if (C == 3) {
            float r = pixel_data[0];
            float g = pixel_data[1];
            float b_val = pixel_data[2];

            if (a_is_broadcast) {
                r = op(channel_vals[0], r);
                g = op(channel_vals[1], g);
                b_val = op(channel_vals[2], b_val);
            } else {
                r = op(r, channel_vals[0]);
                g = op(g, channel_vals[1]);
                b_val = op(b_val, channel_vals[2]);
            }

            c[pixel_offset + 0] = r;
            c[pixel_offset + 1] = g;
            c[pixel_offset + 2] = b_val;
            return;
        }

        // FAST PATH: Power-of-2 channels (8, 16, 32, 64, ...) with vectorization
        // Use float4 for every 4 channels (reduces loads by 4×)
        if ((C % 4 == 0) && (C >= 8) && (C <= 128)) {
            // Check alignment for float4 vectorization
            bool pixel_aligned = (reinterpret_cast<uintptr_t>(pixel_data) % 16) == 0;
            bool chan_aligned = (reinterpret_cast<uintptr_t>(channel_vals) % 16) == 0;
            bool out_aligned = (reinterpret_cast<uintptr_t>(&c[pixel_offset]) % 16) == 0;

            if (pixel_aligned && chan_aligned && out_aligned) {
                const size_t num_vec = C / 4; // Number of float4 loads needed

                // Unroll loop for common sizes (C=8, 16, 32, 64)
                if (C <= 64) {
#pragma unroll
                    for (size_t v = 0; v < num_vec; ++v) {
                        float4 pixel_vals = reinterpret_cast<const float4*>(pixel_data)[v];
                        float4 chan_vals = reinterpret_cast<const float4*>(channel_vals)[v];

                        float4 result;
                        if (a_is_broadcast) {
                            result.x = op(chan_vals.x, pixel_vals.x);
                            result.y = op(chan_vals.y, pixel_vals.y);
                            result.z = op(chan_vals.z, pixel_vals.z);
                            result.w = op(chan_vals.w, pixel_vals.w);
                        } else {
                            result.x = op(pixel_vals.x, chan_vals.x);
                            result.y = op(pixel_vals.y, chan_vals.y);
                            result.z = op(pixel_vals.z, chan_vals.z);
                            result.w = op(pixel_vals.w, chan_vals.w);
                        }

                        reinterpret_cast<float4*>(&c[pixel_offset])[v] = result;
                    }
                } else {
                    // No unrolling for very large C (> 64)
                    for (size_t v = 0; v < num_vec; ++v) {
                        float4 pixel_vals = reinterpret_cast<const float4*>(pixel_data)[v];
                        float4 chan_vals = reinterpret_cast<const float4*>(channel_vals)[v];

                        float4 result;
                        if (a_is_broadcast) {
                            result.x = op(chan_vals.x, pixel_vals.x);
                            result.y = op(chan_vals.y, pixel_vals.y);
                            result.z = op(chan_vals.z, pixel_vals.z);
                            result.w = op(chan_vals.w, pixel_vals.w);
                        } else {
                            result.x = op(pixel_vals.x, chan_vals.x);
                            result.y = op(pixel_vals.y, chan_vals.y);
                            result.z = op(pixel_vals.z, chan_vals.z);
                            result.w = op(pixel_vals.w, chan_vals.w);
                        }

                        reinterpret_cast<float4*>(&c[pixel_offset])[v] = result;
                    }
                }
                return;
            }
            // Fall through to scalar path if unaligned
        }

        // GENERIC PATH: Any channel count (small C or unaligned)
        // Use loop unrolling for small C (< 16)
        if (C <= 16) {
#pragma unroll
            for (size_t ch = 0; ch < C; ++ch) {
                c[pixel_offset + ch] = a_is_broadcast ? op(channel_vals[ch], pixel_data[ch]) : op(pixel_data[ch], channel_vals[ch]);
            }
        } else {
            // No unrolling for large C without vectorization
            for (size_t ch = 0; ch < C; ++ch) {
                c[pixel_offset + ch] = a_is_broadcast ? op(channel_vals[ch], pixel_data[ch]) : op(pixel_data[ch], channel_vals[ch]);
            }
        }
    }

    // Channel3D broadcast kernel WITH COALESCED MEMORY ACCESS
    // Pattern: (H×W×C) op (1×1×C) - OPTIMIZED for memory coalescing
    // Each WARP processes one pixel, threads cooperatively handle channels
    // This achieves COALESCED access: threads in warp load CONSECUTIVE channels!
    template <typename BinaryOp>
    __global__ void broadcast_channel3d_coalesced_kernel_float(
        const float* __restrict__ a,
        const float* __restrict__ b,
        float* __restrict__ c,
        size_t H, size_t W, size_t C,
        bool a_is_broadcast,
        BinaryOp op) {
        // Shared memory for broadcast channels
        extern __shared__ float s_channels[];

        const float* channel_vals = a_is_broadcast ? a : b;
        const float* pixel_data = a_is_broadcast ? b : a;

        // Load channels into shared memory with vectorization
        for (size_t ch = threadIdx.x * 4; ch + 3 < C; ch += blockDim.x * 4) {
            float4 vals = reinterpret_cast<const float4*>(channel_vals)[ch / 4];
            reinterpret_cast<float4*>(s_channels)[ch / 4] = vals;
        }
        // Handle remainder
        for (size_t ch = (C / 4) * 4 + threadIdx.x; ch < C; ch += blockDim.x) {
            s_channels[ch] = channel_vals[ch];
        }
        __syncthreads();

        // Each warp processes different pixels
        const int warp_id = threadIdx.x / 32;
        const int lane_id = threadIdx.x % 32;
        const int num_warps = blockDim.x / 32;
        const size_t total_pixels = H * W;

        // Assign pixels to warps
        for (size_t pixel = blockIdx.x * num_warps + warp_id;
             pixel < total_pixels;
             pixel += gridDim.x * num_warps) {
            const size_t pixel_offset = pixel * C;

            // Process channels in groups of 32 (one per lane)
            for (size_t ch_base = 0; ch_base < C; ch_base += 32) {
                const size_t ch = ch_base + lane_id;

                if (ch < C) {
                    // COALESCED LOAD: All 32 threads load consecutive channels
                    const float pix_val = pixel_data[pixel_offset + ch];
                    const float chan_val = s_channels[ch];

                    // Compute result
                    const float result = a_is_broadcast ? op(chan_val, pix_val) : op(pix_val, chan_val);

                    // COALESCED STORE: All 32 threads write consecutive channels
                    c[pixel_offset + ch] = result;
                }
            }
        }
    }

    // Channel3D broadcast kernel WITH SHARED MEMORY - llm.c optimization pattern
    // Pattern: (H×W×C) op (1×1×C) where C is small (< 3K floats = 12KB)
    // Loads channel vector ONCE into shared memory, all threads reuse from fast local memory
    // Expected speedup: 2-3× vs global memory version for C <= 128
    template <typename BinaryOp>
    __global__ void broadcast_channel3d_smem_kernel_float(
        const float* a, const float* b, float* c,
        size_t H, size_t W, size_t C, bool a_is_broadcast,
        BinaryOp op) {
        // STEP 1: Allocate shared memory for channel vector
        extern __shared__ float s_channels[];

        const float* channel_vals_global = a_is_broadcast ? a : b;

        // STEP 2: Cooperatively load channel vector into shared memory with VECTORIZATION
        // Use float4 (128-bit) loads when possible for better bandwidth utilization
        const size_t num_vec4 = C / 4;

        // Vectorized loads for aligned portion (C must be multiple of 4)
        if (C % 4 == 0 && (reinterpret_cast<uintptr_t>(channel_vals_global) % 16) == 0) {
            // Load 4 floats per thread using float4
            for (size_t v = threadIdx.x; v < num_vec4; v += blockDim.x) {
                float4 vals = reinterpret_cast<const float4*>(channel_vals_global)[v];
                reinterpret_cast<float4*>(s_channels)[v] = vals;
            }
        } else {
            // Scalar loads for unaligned or non-multiple-of-4 cases
            for (size_t ch = threadIdx.x; ch < C; ch += blockDim.x) {
                s_channels[ch] = channel_vals_global[ch];
            }
        }
        __syncthreads(); // Ensure all threads see the loaded data

        // STEP 3: Process pixels (each thread processes one pixel)
        const size_t pixel_idx = (blockIdx.y * gridDim.x + blockIdx.x) * blockDim.x + threadIdx.x;
        const size_t total_pixels = H * W;

        if (pixel_idx >= total_pixels)
            return;

        const size_t pixel_offset = pixel_idx * C;
        const float* pixel_data = a_is_broadcast ? b + pixel_offset : a + pixel_offset;

        // STEP 4: Apply operation using FAST shared memory access!

        // FAST PATH: RGBA with float4 vectorization (most efficient!)
        if (C == 4) {
            // Check if pointers are 16-byte aligned for float4
            bool pixel_aligned = (reinterpret_cast<uintptr_t>(pixel_data) % 16) == 0;
            bool out_aligned = (reinterpret_cast<uintptr_t>(&c[pixel_offset]) % 16) == 0;
            // Shared memory is always aligned to 16 bytes

            if (pixel_aligned && out_aligned) {
                // Vectorized load: 4 floats in one transaction!
                float4 pixel_vals = *reinterpret_cast<const float4*>(pixel_data);
                float4 chan_vals = *reinterpret_cast<const float4*>(s_channels);

                float4 result;
                if (a_is_broadcast) {
                    result.x = op(chan_vals.x, pixel_vals.x);
                    result.y = op(chan_vals.y, pixel_vals.y);
                    result.z = op(chan_vals.z, pixel_vals.z);
                    result.w = op(chan_vals.w, pixel_vals.w);
                } else {
                    result.x = op(pixel_vals.x, chan_vals.x);
                    result.y = op(pixel_vals.y, chan_vals.y);
                    result.z = op(pixel_vals.z, chan_vals.z);
                    result.w = op(pixel_vals.w, chan_vals.w);
                }

                *reinterpret_cast<float4*>(&c[pixel_offset]) = result;
                return;
            }
            // Fallback to scalar for unaligned RGBA
        }

        // FAST PATH: RGB (most common case) - fully unrolled
        if (C == 3) {
            float r = pixel_data[0];
            float g = pixel_data[1];
            float b_val = pixel_data[2];

            // Read from SHARED MEMORY (100× faster than global!)
            if (a_is_broadcast) {
                r = op(s_channels[0], r);
                g = op(s_channels[1], g);
                b_val = op(s_channels[2], b_val);
            } else {
                r = op(r, s_channels[0]);
                g = op(g, s_channels[1]);
                b_val = op(b_val, s_channels[2]);
            }

            c[pixel_offset + 0] = r;
            c[pixel_offset + 1] = g;
            c[pixel_offset + 2] = b_val;
            return;
        }

        // FAST PATH: Power-of-2 channels (8, 16, 32, 64, ...) with vectorization
        // Use float4 for every 4 channels (reduces loads by 4×)
        if ((C % 4 == 0) && (C >= 8) && (C <= 128)) {
            // Check alignment for float4 vectorization
            bool pixel_aligned = (reinterpret_cast<uintptr_t>(pixel_data) % 16) == 0;
            bool out_aligned = (reinterpret_cast<uintptr_t>(&c[pixel_offset]) % 16) == 0;

            if (pixel_aligned && out_aligned) {
                const size_t num_vec = C / 4; // Number of float4 loads needed

                // Unroll loop for common sizes (C=8, 16, 32, 64)
                if (C <= 64) {
#pragma unroll
                    for (size_t v = 0; v < num_vec; ++v) {
                        float4 pixel_vals = reinterpret_cast<const float4*>(pixel_data)[v];
                        // SHARED MEMORY ACCESS (100× faster than global!)
                        float4 chan_vals = reinterpret_cast<const float4*>(s_channels)[v];

                        float4 result;
                        if (a_is_broadcast) {
                            result.x = op(chan_vals.x, pixel_vals.x);
                            result.y = op(chan_vals.y, pixel_vals.y);
                            result.z = op(chan_vals.z, pixel_vals.z);
                            result.w = op(chan_vals.w, pixel_vals.w);
                        } else {
                            result.x = op(pixel_vals.x, chan_vals.x);
                            result.y = op(pixel_vals.y, chan_vals.y);
                            result.z = op(pixel_vals.z, chan_vals.z);
                            result.w = op(pixel_vals.w, chan_vals.w);
                        }

                        reinterpret_cast<float4*>(&c[pixel_offset])[v] = result;
                    }
                } else {
                    // No unrolling for very large C (> 64)
                    for (size_t v = 0; v < num_vec; ++v) {
                        float4 pixel_vals = reinterpret_cast<const float4*>(pixel_data)[v];
                        // SHARED MEMORY ACCESS (100× faster than global!)
                        float4 chan_vals = reinterpret_cast<const float4*>(s_channels)[v];

                        float4 result;
                        if (a_is_broadcast) {
                            result.x = op(chan_vals.x, pixel_vals.x);
                            result.y = op(chan_vals.y, pixel_vals.y);
                            result.z = op(chan_vals.z, pixel_vals.z);
                            result.w = op(chan_vals.w, pixel_vals.w);
                        } else {
                            result.x = op(pixel_vals.x, chan_vals.x);
                            result.y = op(pixel_vals.y, chan_vals.y);
                            result.z = op(pixel_vals.z, chan_vals.z);
                            result.w = op(pixel_vals.w, chan_vals.w);
                        }

                        reinterpret_cast<float4*>(&c[pixel_offset])[v] = result;
                    }
                }
                return;
            }
            // Fall through to scalar path if unaligned
        }

        // GENERIC PATH: Any channel count (small C or unaligned)
        // Use loop unrolling for small C (< 16)
        if (C <= 16) {
#pragma unroll
            for (size_t ch = 0; ch < C; ++ch) {
                // SHARED MEMORY ACCESS (100× faster than global!)
                c[pixel_offset + ch] = a_is_broadcast ? op(s_channels[ch], pixel_data[ch]) : op(pixel_data[ch], s_channels[ch]);
            }
        } else {
            // No unrolling for large C without vectorization
            for (size_t ch = 0; ch < C; ++ch) {
                // SHARED MEMORY ACCESS (100× faster than global!)
                c[pixel_offset + ch] = a_is_broadcast ? op(s_channels[ch], pixel_data[ch]) : op(pixel_data[ch], s_channels[ch]);
            }
        }
    }

    // Batch Broadcast 3D kernel - VECTORIZED for neural network patterns
    // Pattern: (B×H×W) op (1×H×W) - broadcast H×W plane across B batches
    // Common in batch normalization, instance normalization
    template <typename BinaryOp>
    __global__ void broadcast_batch3d_kernel_float(
        const float* a, const float* b, float* c,
        size_t B, size_t H, size_t W, bool a_is_broadcast,
        BinaryOp op) {
        const size_t batch = blockIdx.y;
        const size_t elem_vec = blockIdx.x * blockDim.x + threadIdx.x;
        const size_t elem = elem_vec * 4; // Process 4 elements per thread

        const size_t plane_size = H * W;
        if (batch >= B || elem >= plane_size)
            return;

        const size_t batch_offset = batch * plane_size;
        const float* batch_data = a_is_broadcast ? b + batch_offset : a + batch_offset;
        const float* broadcast_plane = a_is_broadcast ? a : b;

        // Check alignment for float4 vectorization
        bool plane_aligned = (reinterpret_cast<uintptr_t>(broadcast_plane) % 16) == 0;
        bool data_aligned = (reinterpret_cast<uintptr_t>(batch_data) % 16) == 0;
        bool out_aligned = (reinterpret_cast<uintptr_t>(&c[batch_offset]) % 16) == 0;

        // Vectorized path: Load 4 floats in one transaction
        if (plane_aligned && data_aligned && out_aligned && elem + 3 < plane_size) {
            float4 broadcast_vals = reinterpret_cast<const float4*>(broadcast_plane)[elem_vec];
            float4 data_vals = reinterpret_cast<const float4*>(batch_data)[elem_vec];

            float4 result;
            if (a_is_broadcast) {
                result.x = op(broadcast_vals.x, data_vals.x);
                result.y = op(broadcast_vals.y, data_vals.y);
                result.z = op(broadcast_vals.z, data_vals.z);
                result.w = op(broadcast_vals.w, data_vals.w);
            } else {
                result.x = op(data_vals.x, broadcast_vals.x);
                result.y = op(data_vals.y, broadcast_vals.y);
                result.z = op(data_vals.z, broadcast_vals.z);
                result.w = op(data_vals.w, broadcast_vals.w);
            }

            reinterpret_cast<float4*>(&c[batch_offset])[elem_vec] = result;
        }
        // Scalar fallback for unaligned data or remainder
        else if (elem < plane_size) {
            for (size_t i = elem; i < plane_size && i < elem + 4; ++i) {
                const size_t idx = batch_offset + i;
                c[idx] = a_is_broadcast ? op(broadcast_plane[i], batch_data[i]) : op(batch_data[i], broadcast_plane[i]);
            }
        }
    }

    // ============================================================================
    // BINARY BROADCAST KERNEL (Generic fallback)
    // ============================================================================

    template <typename T, typename OutputT, typename BinaryOp>
    __global__ void broadcast_binary_kernel(
        const T* a, const T* b, OutputT* c,
        const int* a_shape, const int* b_shape, const int* c_shape,
        int a_rank, int b_rank, int c_rank,
        size_t c_elements, BinaryOp op) {
        size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx >= c_elements)
            return;

        // Compute strides inline
        int c_strides[8], a_strides[8], b_strides[8];

        c_strides[c_rank - 1] = 1;
        for (int i = c_rank - 2; i >= 0; --i) {
            c_strides[i] = c_strides[i + 1] * c_shape[i + 1];
        }

        if (a_rank > 0) {
            a_strides[a_rank - 1] = 1;
            for (int i = a_rank - 2; i >= 0; --i) {
                a_strides[i] = a_strides[i + 1] * a_shape[i + 1];
            }
        }

        if (b_rank > 0) {
            b_strides[b_rank - 1] = 1;
            for (int i = b_rank - 2; i >= 0; --i) {
                b_strides[i] = b_strides[i + 1] * b_shape[i + 1];
            }
        }

        // Broadcast indexing
        int a_idx = 0, b_idx = 0;
        size_t remaining = idx;

        for (int i = 0; i < c_rank; ++i) {
            int c_coord = remaining / c_strides[i];
            remaining %= c_strides[i];

            // Map to a's coordinate
            int offset_a = c_rank - a_rank;
            if (i >= offset_a) {
                int dim = i - offset_a;
                int coord = (a_shape[dim] == 1) ? 0 : c_coord;
                a_idx += coord * a_strides[dim];
            }

            // Map to b's coordinate
            int offset_b = c_rank - b_rank;
            if (i >= offset_b) {
                int dim = i - offset_b;
                int coord = (b_shape[dim] == 1) ? 0 : c_coord;
                b_idx += coord * b_strides[dim];
            }
        }

        c[idx] = op(a[a_idx], b[b_idx]);
    }

#endif // __CUDACC__

    // ============================================================================
    // BINARY BROADCAST HOST LAUNCHER TEMPLATE (INLINE FOR CORRECT INSTANTIATION)
    // ============================================================================

    template <typename T, typename OutputT, typename BinaryOp>
    void launch_broadcast_binary(const T* a, const T* b, OutputT* c,
                                 const size_t* a_shape, const size_t* b_shape, const size_t* c_shape,
                                 size_t a_rank, size_t b_rank, size_t c_rank,
                                 size_t c_elements, BinaryOp op, cudaStream_t stream) {
        if (c_elements == 0)
            return;

        // DEBUG: Disabled for production - enable only for debugging
        // fprintf(stderr, "BROADCAST CALLED: c_elem=%zu, a_rank=%zu, b_rank=%zu\n", c_elements, a_rank, b_rank);
        // fflush(stderr);

        // Use specialized kernels for float→float operations
        constexpr bool use_vectorized_float = std::is_same<T, float>::value && std::is_same<OutputT, float>::value;

        // Use specialized comparison kernels for float→unsigned char operations
        constexpr bool use_vectorized_comparison = std::is_same<T, float>::value && std::is_same<OutputT, unsigned char>::value;

        // FAST PATH: Comparison operations (float→unsigned char)
        if constexpr (use_vectorized_comparison) {
            // Detect broadcast pattern for optimization
            BroadcastPattern pattern = detect_broadcast_pattern(a_shape, b_shape, a_rank, b_rank, c_rank);
            const int block_size = 256;

            // Currently only Row pattern is optimized for comparisons
            // Other patterns fall through to generic kernel
            if (pattern == BroadcastPattern::Row) {
                const size_t M = c_shape[0];
                const size_t N = c_shape[1];
                const bool a_is_row = (a_shape[0] == 1);

                // Each thread processes 4 elements
                const size_t num_vec = (N + 3) / 4;
                dim3 grid((num_vec + block_size - 1) / block_size, M);

#ifdef __CUDACC__
                broadcast_row_comparison_kernel<<<grid, block_size, 0, stream>>>(
                    a, b, c, M, N, a_is_row, op);
#else
                static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif
                return;
            }
            // For other patterns, fall through to generic kernel
        }

        if constexpr (use_vectorized_float) {
            // Detect broadcast pattern for optimization
            BroadcastPattern pattern = detect_broadcast_pattern(a_shape, b_shape, a_rank, b_rank, c_rank);
            const int block_size = 256;

            // DEBUG: Disabled for production
            // const char* pattern_names[] = {"Scalar", "Row", "Column", "Channel3D", "Generic"};
            // printf("BROADCAST: Pattern=%s, c_elem=%zu, a_rank=%zu, b_rank=%zu, c_rank=%zu\n",
            //        pattern_names[static_cast<int>(pattern)], c_elements, a_rank, b_rank, c_rank);

            switch (pattern) {
            case BroadcastPattern::Scalar: {
                // Scalar broadcast: (M×N) op scalar
                size_t a_size = 1, b_size = 1;
                for (size_t i = 0; i < a_rank; ++i)
                    a_size *= a_shape[i];
                for (size_t i = 0; i < b_rank; ++i)
                    b_size *= b_shape[i];

                const int grid_size = (c_elements + block_size - 1) / block_size;

#ifdef __CUDACC__
                broadcast_scalar_kernel_float<<<grid_size, block_size, 0, stream>>>(
                    a, b, c, a_size, b_size, c_elements, op);
#else
                static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif
                return;
            }

            case BroadcastPattern::Row: {
                // Row broadcast: (M×N) op (1×N) - VECTORIZED with float4!
                const size_t M = c_shape[0];
                const size_t N = c_shape[1];
                const bool a_is_row = (a_shape[0] == 1);

                // Each thread processes 4 elements
                const size_t num_vec = (N + 3) / 4;
                dim3 grid((num_vec + block_size - 1) / block_size, M);

#ifdef __CUDACC__
                broadcast_row_kernel_float<<<grid, block_size, 0, stream>>>(
                    a, b, c, M, N, a_is_row, op);
#else
                static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif
                return;
            }

            case BroadcastPattern::Column: {
                // Column broadcast: (M×N) op (M×1)
                const size_t M = c_shape[0];
                const size_t N = c_shape[1];
                const bool a_is_col = (a_shape[1] == 1);

                dim3 grid((N + block_size - 1) / block_size, M);

#ifdef __CUDACC__
                broadcast_column_kernel_float<<<grid, block_size, 0, stream>>>(
                    a, b, c, M, N, a_is_col, op);
#else
                static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif
                return;
            }

            case BroadcastPattern::Channel3D: {
                // Channel3D broadcast: (H×W×C) op (1×1×C) - VECTORIZED FAST PATH!
                // Critical for neural rendering: color transforms, normalization, etc.
                const size_t H = c_shape[0];
                const size_t W = c_shape[1];
                const size_t C = c_shape[2];
                const bool a_is_broadcast = (a_shape[0] == 1 && a_shape[1] == 1);

                // Grid: process all pixels in parallel
                const size_t total_pixels = H * W;
                const int grid_size = (total_pixels + block_size - 1) / block_size;
                const int max_grid_dim = 65535; // CUDA limit

                // Use 2D grid if 1D grid would exceed limit
                dim3 grid;
                if (grid_size <= max_grid_dim) {
                    grid = dim3(grid_size, 1);
                } else {
                    // Split into 2D grid
                    const int grid_x = (grid_size + max_grid_dim - 1) / max_grid_dim;
                    const int grid_y = (grid_size + grid_x - 1) / grid_x;
                    grid = dim3(grid_x, grid_y);
                }

                // NOTE: For C >= 16, memory coalescing becomes an issue with (H×W×C) layout
                // Each thread processes one pixel (all C channels), causing strided access
                // PyTorch likely uses different layout or transpose for better coalescing
                // Our kernel is optimized for small C (3,4,8) which are most common in rendering
                // For C=64, we're ~20% slower than PyTorch, but 10-12× faster for C=3,4!

#ifdef __CUDACC__
                broadcast_channel3d_kernel_float<<<grid, block_size, 0, stream>>>(
                    a, b, c, H, W, C, a_is_broadcast, op);
#else
                static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif
                return;
            }

            case BroadcastPattern::BatchBroadcast3D: {
                // Batch broadcast: (B×H×W) op (1×H×W) - VECTORIZED with float4!
                // Common in batch normalization, instance normalization
                const size_t B = c_shape[0];
                const size_t H = c_shape[1];
                const size_t W = c_shape[2];
                const bool a_is_broadcast = (a_shape[0] == 1);

                const size_t plane_size = H * W;
                const size_t num_vec = (plane_size + 3) / 4; // Each thread processes 4 elements
                dim3 grid((num_vec + block_size - 1) / block_size, B);

#ifdef __CUDACC__
                broadcast_batch3d_kernel_float<<<grid, block_size, 0, stream>>>(
                    a, b, c, B, H, W, a_is_broadcast, op);
#else
                static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif
                return;
            }

            case BroadcastPattern::Generic:
                // Fall through to generic kernel
                break;
            }
        }

        // Generic kernel for all types and complex patterns
        const int block_size = 256;

        // Generic N-D broadcast - use existing implementation
        int* d_a_shape = static_cast<int*>(
            CudaMemoryPool::instance().allocate(a_rank * sizeof(int), stream));
        int* d_b_shape = static_cast<int*>(
            CudaMemoryPool::instance().allocate(b_rank * sizeof(int), stream));
        int* d_c_shape = static_cast<int*>(
            CudaMemoryPool::instance().allocate(c_rank * sizeof(int), stream));

        if (!d_a_shape || !d_b_shape || !d_c_shape) {
            LOG_ERROR("Failed to allocate shape arrays from memory pool");
            if (d_a_shape)
                CudaMemoryPool::instance().deallocate(d_a_shape, stream);
            if (d_b_shape)
                CudaMemoryPool::instance().deallocate(d_b_shape, stream);
            if (d_c_shape)
                CudaMemoryPool::instance().deallocate(d_c_shape, stream);
            return;
        }

        std::vector<int> a_vec(a_shape, a_shape + a_rank);
        std::vector<int> b_vec(b_shape, b_shape + b_rank);
        std::vector<int> c_vec(c_shape, c_shape + c_rank);

        cudaMemcpy(d_a_shape, a_vec.data(), a_rank * sizeof(int), cudaMemcpyHostToDevice);
        cudaMemcpy(d_b_shape, b_vec.data(), b_rank * sizeof(int), cudaMemcpyHostToDevice);
        cudaMemcpy(d_c_shape, c_vec.data(), c_rank * sizeof(int), cudaMemcpyHostToDevice);

        const int grid_size = (c_elements + block_size - 1) / block_size;

#ifdef __CUDACC__
        broadcast_binary_kernel<<<grid_size, block_size, 0, stream>>>(
            a, b, c, d_a_shape, d_b_shape, d_c_shape,
            a_rank, b_rank, c_rank, c_elements, op);
#else
        static_assert(sizeof(T) == 0, "CUDA compiler required for broadcast operations");
#endif

        CudaMemoryPool::instance().deallocate(d_a_shape, stream);
        CudaMemoryPool::instance().deallocate(d_b_shape, stream);
        CudaMemoryPool::instance().deallocate(d_c_shape, stream);
    }

} // namespace gs::tensor_ops
