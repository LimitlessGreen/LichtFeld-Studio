/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 *
 * SPDX-License-Identifier: GPL-3.0-or-later */

#pragma once

#include "core_new/tensor.hpp"
#include <expected>
#include <string>

namespace lfs::training::losses {

    /**
     * @brief L1 regularization on exp(scaling_raw) with fused CUDA kernel
     *
     * Forward:  scaling = exp(scaling_raw)
     * Loss:     L = weight * mean(scaling)
     * Gradient: ∂L/∂scaling_raw = (weight / N) * exp(scaling_raw)
     *
     * NOTE: This loss writes gradients directly to scaling_raw_grad in-place
     */
    struct ScaleRegularization {
        struct Params {
            float weight; ///< Regularization weight
        };

        /**
         * @brief Compute scale regularization loss and accumulate gradients
         * @param scaling_raw [N, 3] raw scaling parameters
         * @param scaling_raw_grad [N, 3] gradient tensor (will be accumulated to)
         * @param params Loss parameters
         * @return loss_value or error
         * @note Accumulates gradients directly to scaling_raw_grad
         */
        static std::expected<float, std::string> forward(
            const lfs::core::Tensor& scaling_raw,
            lfs::core::Tensor& scaling_raw_grad,
            const Params& params);
    };

    /**
     * @brief L1 regularization on sigmoid(opacity_raw) with fused CUDA kernel
     *
     * Forward:  opacity = sigmoid(opacity_raw)
     * Loss:     L = weight * mean(opacity)
     * Gradient: ∂L/∂opacity_raw = (weight / N) * sigmoid(x) * (1 - sigmoid(x))
     *
     * NOTE: This loss writes gradients directly to opacity_raw_grad in-place
     */
    struct OpacityRegularization {
        struct Params {
            float weight; ///< Regularization weight
        };

        /**
         * @brief Compute opacity regularization loss and accumulate gradients
         * @param opacity_raw [N, 1] raw opacity parameters
         * @param opacity_raw_grad [N, 1] gradient tensor (will be accumulated to)
         * @param params Loss parameters
         * @return loss_value or error
         * @note Accumulates gradients directly to opacity_raw_grad
         */
        static std::expected<float, std::string> forward(
            const lfs::core::Tensor& opacity_raw,
            lfs::core::Tensor& opacity_raw_grad,
            const Params& params);
    };

} // namespace lfs::training::losses
